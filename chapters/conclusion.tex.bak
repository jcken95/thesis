\begin{chapter}{Conclusion \label{Ch:conclusions}}

The goal of this thesis was to contribute to the toolkit of the subjective Bayesian; in particular we wished to reduce the computational burden of problems a subjective Bayesian may encounter when working with complex, stochastic simulators.

The contributions of this thesis were application-driven; we considered two problems within an offshore wind farm setting. The first problem was performing a meaningful sensitivity analysis so that a facilitator could prioritise the most important parameters for a SHELF elicitation workshop. This relied on constructing a data-hungry HetGP emulator. To improve the predictive properties of the HetGP emulators in a low-data regime, we developed and constructed the SML emulator. The next problem we considered was performing decision support so that a DM could devise a plan for managing a warehouse containing spare parts to be used when subassemblies suffer serious failures. The computational cost of each problem was greatly reduced by replacing the Athena simulator by suitably constructed emulators. Our emulators allowed us to perform decision support for stochastic simulators, we also considered sampling from NROY regions when the decision space is a discrete simplex.

\section{Thesis summary}
We began by introducing the Athena simulator and explaining some of its key components. The standard version of Athena overlooks the fact that spare components for key subassemblies may well be in limited supply. We therefore developed, and incorporated, some additional mechanisms which take into account that spare parts are difficult to obtain. We established that in situations where relevant data are difficult --- or even impossible --- to obtain, probability elicitation offers a practical solution to the problem of quantifying parameter uncertainty. We therefore discussed the SHELF; a framework for eliciting probability distributions which allows for, and encourages, the use of many experts for parameter elicitation. In a similar vein, when it is not clear what decision is optimal, we can elicit a utility function from a DM. This motivated our discussion of utility elicitation within the wider context of Bayesian decision analysis.

We then formally introduced the Gaussian Process, a stochastic process which naturally lends itself as a prior distribution for functions. The GP leads us to the notion of an emulator; a fast statistical surrogate model which produces a central prediction for a complex simulator with an appropriate quantification of uncertainty attached. We discussed a handful of common covariance functions and showed that (Bayesian) linear regression is linked to GP regression via linear covariance functions --- this fact can be used to improve the interpretability of emulators as the sum of two or more independent GPs is also a GP. We also discussed two other approaches to emulation. Linear regression offers a highly efficient (but perhaps less accurate) approach to emulation. Bayes linear approaches are also well suited to emulation; Bayes linear emulators are mathematically very similar to GPs, but offer a more robust analysis when we believe that a GP approximation of the simulator is too restrictive. These emulators are useful as they do not tie us to any probabilistic beliefs about how we expect an unknown function to behave. The catch is that, within a Bayes linear framework, we are limited in the types of statements we can make about unknowns.

We then explored methods for simultaneous mean and variance emulation. Simultaneous approaches, like HetGP, are more satisfying than the two-emulator approach \citep{Henderson09,Andrianakis2017} as we only need one surrogate for a single quantity of interest. A shortcoming of both of these types of models is that they require a lot of information to produce good quality emulators. This was avoided by the development of the SML emulator which exploited properties of Athena (the ability to run Athena at varying levels of complexity) to construct an improved emulator. We verified that the SML emulator offered an improvement by quantitative measures (RMSE and an appropriate scoring rule), as well as graphical diagnostics  suggesting a reduced emulator-simulator discrepancy under SML.

We then used our novel emulator to gain a deeper understanding of how a subset of the Athena simulator's parameters impact the output of the Athena simulator. We performed a probabalistic sensitivity analysis to see how changing the mean time to degradation impacted the distribution of the mean availability. We found that two parameters were responsible for the majority of the variation in the mean probit availability; the mean time to degradation for the turbine blades and the generator. The parameters were all of roughly equal importance for the variance component of Athena.

We then returned to decision analysis and reviewed various optimisation methods. Since the Athena simulator is computationally expensive, BayesOpt offered a promising solution to maximising a function of the outputs of the Athena simulator. In particular, BayesOpt allows us to maximise an expected utility function which depends on Athena. We also discussed that, because utility functions, elicited beliefs and models will always be imperfect representations of the objects they aim to represent, that allowing the analyst to make the decision, via performing mathematical optimisation, is myopic. The analyst in the problem should only \textit{support} the decision making process; it is up to the DM to make the decision. One way to do this is to employ history matching inspired techniques to construct a set of decisions which are, given all relevant and specified uncertainties, consistent with the maximiser. %In our case study we only have access to an approximate maximiser, so we found a set of decisions that were consistent with this approximate maximiser by considering the additional uncertainty about the approximate maximum value of the elicited utility function.

We then applied a novel combination of BayesOpt and history matching inspired methods to construct a set of decisions to aid a logistics problem. The Athena simulator featured in the problem by incorporating the availability time series into the elicited utility function. In our particular problem, we managed to greatly simplify the DM's decision by reducing the `big' warehouse decision down from a selection of $3$ warehouses to $2$. We also found decisions within each warehouse that were consistent with the approximate maximiser. Our volume of the final set of decisions was around $0.2\%$ of the size of the original decision space. The final set of decisions all lead to reasonably similar availability trajectories from the Athena simulator, which tells us that the DM can choose a warehouse and management policy from the final set of decisions that suits their preferences, without adversely affecting wind farm performance.

\section{Future research avenues}
\subsection{HetGP, SML and sensitivity analysis}
We showed that our novel SML emulator provided an improved emulator, over the benchmark HetGP, for our motivating Athena example at a fixed training budget. This allowed us to perform a sensitivity analysis to determine the relative importance of a subset of parameters to the Athena simulator.

The design for this example was chosen to be space filling, but beyond this, had no special properties. A promising avenue for future research would be developing designs which leverage our knowledge about the multiple levels of code.  Plainly, we should explore sequential designs. Although one `expensive' run may be computationally equivalent to $t$ `cheap' runs, it is not clear how informative cheap runs really are for expensive runs. Minimising some criterion, such as integrated mean squared prediction error, would allow the emulator to choose which of (i) a single expensive run or (ii) multiple cheap runs would be most useful for predicting the output of Athena (or some other simulator). This would also allow us to incorporate replication within the design. Replication is encouraged when emulating stochastic simulators with GPs since the cost and inference of GP modelling depends on the number of unique design \textit{locations}, $n$, rather than the total number of simulator runs, $N$. If $n << N$ then large computational savings are available due to the cubic/quadratic nature of GP calculations. This allows us to maximise the amount learned about $y(\cdot)$ whilst minimising the cost of implementing our emulators. Another idea that could be explored is incorporating more levels. The autoregressive function structure offers a natural way to incorporate many levels of code \citep{Kennedy2000}; we could also use this method to see which level of code is most time effective. Recall that an autoregressive model for functions takes the form
\begin{equation}
  f_{t+1}(\cdot) = \rho_{t} f_t(\cdot) + \delta_{t}(\cdot)
\end{equation}
where $t$ is a code level, $\rho_t$ is a regression parameter and $\delta_t(\cdot)$ is a discrepancy term. If $\delta_t(\cdot)$ is close to zero, and $\rho_t \approx 1$, then this suggests that $f_t(\bx) \approx f_{t+1}(\bx)$ and thus there is little advantage in using $f_{t+1}(\cdot)$ over $f_t(\cdot)$. The stochastic case should also consider if the output stochasticity is similar in the various code levels.

We would also like to consider ways to assess the joint impact of inputs on the simulator distribution. We mentioned in the discussion of \cref{Ch:sensitivity} that EVPI based methods would naturally allow for this when we have access to an appropriate utility function. Off the shelf utility functions, such as those based on the Kullback-Leibler divergence, may offer a good `default' when we are either unsure of what the utility function should be, or are interested in using the model for tasks such as parameter inference. Borrowing commonly used utility functions from the Bayesian design of experiments literature offers a promising start.

\subsection{Decision support: multiple stakeholders}

Using history matching techniques for decision support could be expanded to the case of multiple decision makers (typically termed ``stakeholders'') with different perspectives. Current approaches are somewhat limiting. For example, \citet{Keller2009} review several decision problems with multiple stakeholders. Typical solutions to decision problems with multiple stakeholders involve each stakeholder assigning a score to each possible decision by scoring the possible consequences, which are typically discrete in nature. Notably, such methods do not allow for any quantification of uncertainty about which decision is optimal, or the ability to clearly see whether the stakeholders can come to a compromise or not. Application of the single-stakeholder (a single DM) approach as in \cref{Ch:ds-for-ow} would generalise in a natural way.

Applying a history matching inspired approach, with $E$ stakeholders, we could construct $E$ sets of decisions that are consistent with the $e$th stakeholder's maximiser. Let these sets be $\calX_1,  \calX_2, \ldots, \calX_E$. The intersection of all these sets, $\calX^{*} = \bigcap_{e = 1}^E \calX_e$, is the set of mutually agreeable decisions; those that all stakeholders would be happy to take. Note that the DMs would not have to have the same set of attributes. If a DM, Alice, wanted to include $x_i$ in their analysis, but if a DM, Bob, did not want to include $x_i$, then we can just ensure that Bob's utility function is flat with respect to $x_i$. In an additive form, set $u_i(x_i)=0$, and for a multiplicative form set $u_i(x_i)=1$.

Although this is conceptually no more difficult than applying the approach to a single DM, there are computational and design issues that need to be considered. For example, when  constructing wave $k>1$ designs, we should investigate whether to  construct a design amongst the intersection of each decision maker's NROY set, and evaluate each DM's utility function \textit{only} at the intersection, or whether to construct a design amongst each DM's NROY set and evaluate each DMs utility function at their own set of NROY decisions. The advantage of the first approach is efficiency; if a decision is ruled out as NROY by a single DM, then it would not typically be considered any further. The second approach may be useful when the $U_e(\bx)$, the utility function of each DM, are replaced by emulators. As knowledge about the $U_e(\bx)$ evolves  over different waves of emulation, the intersection of each $\calX_e$ may shrink, but may also grow. An additional consideration that could be built into this framework would be a willingness to compromise. In such a case, the $e$th DM's implausibility function would be of the form
\begin{equation}
  I_e(\bx) = \frac{ \E\{U(\hat{\bx}_e)\} - \E\{U(\bx)\} } {\sqrt{V + \sigma^2_{e,c}}}
\end{equation}
where $V$ represents all the usual sources of uncertainty (for example, emulator variance or model discrepancy), $\hat{\bx}_e$ is the (approximate) maximiser of $U_e(\bx)$ and $\sigma^2_{e, c}$  is a variance-like term which quantifies how willing the $e$th DM is to compromise. It is not clear how to elicit a compromise parameter; this should be given consideration in future research.
\end{chapter}
