\begin{chapter}{Introduction \label{Ch:Intro}}
\section{Motivation: computer experiments}
Historically, science was undertaken almost exclusively via physical experiments. In search of new knowledge, the scientific community did many unusual, and sometimes dangerous, things. Marie Curie's experimentation with X-rays almost surely led to her premature death, Ernest Rutherford (or rather, his students, Hans Geiger and Ernest Marsden), sat in dark rooms for hours in order to prove that atoms are indeed divisible, and to investigate the feasibility of cryonics, \citet{Smith1954} froze hamsters and reanimated them using microwaves.

The scientific process has since evolved. We have developed ethical standards for the types of experiments we are allowed to perform, as well and health and safety protocols. A key advancement has been the development of mathematical and statistical modelling to predict what may happen in complex systems. Such models take inputs, $\bx$, transform them through a function, $f(\cdot)$, and produce an output, $y = f(\bx)$. The function $f(\cdot)$ may encode our understanding of a physical process via a set of complex equations and $\bx$, the parameter settings, may represent a hypothetical scenario. Evaluating $f(\cdot)$ at many values of $\bx$ allows a scientist to ask many ``what if'' questions, at a fraction of the cost of running an equivalent physical experiment. The function $f(\cdot)$ is usually intractable, thus must be evaluated via a computer code. We refer to a computer code as a \textit{simulator}. Typical simulators can be of the order of hundreds, or even thousands, of lines of code. Answering these ``what if'' questions via a computer is what we might term a \textit{computer experiment}. Within the biological community, this form of experiment is sometimes termed an \textit{in silco} experiment.

In many engineering contexts, such as the generation of energy, we encounter physical systems that are incredibly complex. An engineer may want to optimise parameters of these systems, or simply explore what would happen under many ``what if'' scenarios. Performing physical experiments on gas distribution networks, full scale nuclear power plants, or offshore wind farms located miles away from the coast is not a straightforward task. Even if these experiments were financially feasible, the experiments would undoubtedly take \textit{a very long time}.

Using a computer simulation to mimic a physical system is no panacea. Despite modern computing capabilities, it is typical of many computer simulators to have long run times. Exactly what `long' means is context-dependent, but the Athena simulator \citep{Zit13, Zit16, Zit2021}, used throughout this thesis, has typical run times of $5$ minutes for a single parameter setting on a modern desktop computer. A $5$ minute run time might be acceptable for making a handful of runs, but becomes problematic when we want to make thousands of runs. A more extreme example is HadCM3 --- the climate model used by the MET office --- run times are in excess of a week for a single parameter setting \citep{Domingo2020}.

Despite the advantages of using a model in place of physical experiments, we must always remind ourselves that the model is exactly that. It is a model. An imperfect representation of the physical system it aims to mimic.

\subsection{The role of statistical thinking in computer experiments}

Statistics has made, and continues to make, contributions to computer experiments and the wider field of Uncertainty Quantification (UQ). The main role of statistics in this field is the development and application of surrogate models known as \textit{emulators} \citep{Ohagan01, Vernon14}. Emulators take a set of inputs $X$ and evaluations of the simulator at those inputs $\by = f(X)$, to construct a statistical model typically several orders of magnitude faster to run than the original simulator \citep{Salter2016}. The statistical model is an approximation to the simulator, thus an appropriate quantification of uncertainty is attached to the emulator's point predictions.

Statistical thinking does not just allow us to perform efficient prediction, and besides, prediction for its own sake is rarely the objective of a simulator. We typically develop simulators as a means to perform another task, such as optimisation, inference, calibration or decision analysis. Phrasing the use of a complex simulator in statistical terms has lead to many developments. A popular example is \textit{Bayesian Optimisation}, where ideas from statistical decision theory allow us to optimise expensive simulators with relatively few runs of the simulator \citep{Frazier2018}. Sampling schemes such as Latin Hypercubes \citep{Mckay1979}, allow us to construct designs for simulation experiments which cover the input space efficiently which eases the task of emulation. Elicitation and subjectivism allow us to consider model discrepancy and thus allow us to calibrate simulators to observational data, whilst allowing for the fact that the simulator (and emulator) are imperfect representations of a corresponding physical system \citep{Ohagan01, Vernon2022}.

\section{Contributions}
\subsection{Stochastic multilevel emulation}
The first main contribution is the development an emulator that we call a stochastic multilevel (SML) emulator. The SML emulator combines two established approaches; HetGP and multilevel emulation.

HetGP \citep{Goldberg1998, Binois2018} is a data-hungry model, which is otherwise well suited to the emulation of stochastic simulators. It uses a pair of GP emulations to construct flexible mean and variance estimates for stochastic simulators. Multilevel emulation leverages the fact that many simulators can be run at varying levels of complexity, and thus we have a more accurate simulator at higher `levels'. The more accurate/complex the simulator is, the longer it takes to run. The multilevel emulator \citep{Kennedy2000}, finds a way to combine runs from different levels of \textit{deterministic} code, to provide improved predictions of the most expensive level of code. Our SML emulator combines these two approaches to enable us to use cheap versions of a stochastic simulator to construct improved predictions for a more expensive version. We also consider several estimation techniques for HetGP and SML, and find a computationally efficient way to incorporate prior information into both HetGP and SML emulators.

\subsection{A decision support framework for offshore wind}
Our second main contribution is the development of a decision support framework using stochastic simulators. This is an extension of the decision support frameworks given in \citet{Lawson2016} and \citet{Owen2020} who study deterministic simulators. The core idea in decision support is to find decisions (model inputs) which are within a given tolerance of some approximately optimal reference value. Our application has several novel ideas; firstly we discuss how decision making and decision support can be brought closer together via Bayesian optimisation and history matching. Our application also has a challenging feature; a subset of our simulator inputs form a discrete simplex. Within the computer experiments literature, inputs are usually formed on a hypercube. We discuss a method for generating uniformly distributed samples from the discrete simplex which are within tolerance of the reference value. We also consider post-processing of the uniform samples to improve the space-filling properties of a design for a multi-wave computer experiment.

\section{Thesis outline}

In \cref{Ch:background} we introduce the Athena simulator, a complex, stochastic simulator used to model the behaviour of a large offshore wind farm. The purpose of the Athena simulator is to aid decision making under uncertainty. This uncertainty is decomposed into two parts. Firstly, aleatory uncertainty; the inherent randomness of the world. The second is epistemic uncertainty; we represent this mathematically via a (prior) probability distribution. Athena employs a large and complex prior probability distribution over many unknown quantities for which there is little or no relevant data available; many relate to the lifetime distributions of critical components in Athena.

Probability elicitation is important within Athena as many of the parameters are uncertain and thus are represented in the Athena simulator by an expert's prior distribution. We want to propagate the uncertain parameters through the Athena simulator to understand how input uncertainty induces output uncertainty. We therefore discuss some core concepts from probability elicitation. Since Athena is used to help make operation and maintenance decisions within an offshore wind setting, we also provide an introduction to Bayesian decision analysis and utility theory; a framework for making decisions.

The use of computationally expensive models makes analyses relying on many evaluations of $f(\bx)$ extremely computationally expensive. Such analyses include uncertainty analysis or decision analysis. In \cref{Ch:Emulators}, we introduce the notion of an \textit{emulator}. Emulators are fast, statistical approximations to complex computer simulators. Emulators repurpose and adapt regression techniques to produce approximations to complex computer simulators.

In \cref{Ch:Hetsml}, we describe the heteroscedastic Gaussian Process (HetGP) as a way to emulate stochastic computer models which exhibit input-dependent noise. We develop a general approach for SML emulation, which uses different versions of a simulator to improve the final emulator. Finally, we compare some estimation methods for HetGP and SML via an application to the Athena simulator.

We return to probability elicitation in \cref{Ch:sensitivity}. In particular, we perform a probabalistic sensitivity analysis of the Athena simulator. This is facilitated by the HetGP and SML emulators fitted in \cref{Ch:Hetsml}.

\cref{Chap:optimisation} marks a change of direction in the thesis. We review some common optimisation approaches with a view towards decision making (which can be expressed as an optimisation problem). We then discuss how emulators can be incorporated into the optimisation process. We consider some practical differences between \textit{making} decisions and \textit{supporting} decision making. We motivate the use of history matching inspired techniques as an appropriate method for facilitating decision support. Finally, we discuss algorithms for generating sets of decisions that are consistent with an approximately optimal decision.

In \cref{Ch:ds-for-ow}, we put into practice some of the techniques described in \cref{Chap:optimisation}. The optimisation techniques are used to maximise the expected utility function which depends on the output of Athena. We utilise techniques given in \citet{Owen2020} and extend them to the stochastic case. We also consider the problem of generating decisions on a subset of a discrete simplex; this is a nonstandard problem within the uncertainty quantification literature.

The thesis is drawn to a close in \cref{Ch:conclusions}. We review the findings of this thesis and provide future research avenues.
\end{chapter}
